
**1. Task Completion & Functional Accuracy Tests:**

*   **Goal:** Does MIDAS *correctly* perform the requested software development sub-tasks?
*   **Scenarios:**
    *   **Basic End-to-End (Happy Path):**
        *   Give a simple, unambiguous feature request (e.g., "Add a REST endpoint `/ping` that returns `pong`").
        *   **Verify:** Correct JIRA Epic/Story/Task hierarchy created, Confluence page generated with basic spec, code implementing the endpoint written and committed, basic test written and passing, JIRA tickets updated appropriately.
    *   **Specific Agent Functionality:**
        *   **Planner:** Provide a complex spec. Verify the Epics created are logical, distinct, and cover the spec. Check Confluence links.
        *   **PO:** Provide an Epic key. Verify the Stories/Tasks created are actionable, correctly linked, and have placeholders for ACs.
        *   **Coder:** Provide a specific Task (e.g., "Refactor function X to improve readability"). Verify the code changes achieve the goal, pass existing tests, and follow project style guides (use linters).
        *   **Tester:** Provide a Task key with known simple bugs introduced by the Coder agent (or manually). Verify the Tester agent creates appropriate JIRA Bug tickets linked to the Task/Story.
        *   **Architect:** Ask for a sequence diagram for a specific flow described in a Story. Verify the output (e.g., PlantUML text) is generated and accurate.
        *   **DevOps:** Ask to set up a basic CI pipeline file for a simple project type. Verify the generated file (e.g., `.gitlab-ci.yml`, `Jenkinsfile`) is syntactically valid and contains expected stages (build, test).
        *   **Security:** Ask to scan dependencies (`package.json`, `requirements.txt`). Verify known vulnerabilities are flagged (requires a controlled environment/package list).
        *   **UI/UX:** Provide a Story ("Add a login button"). Verify it generates reasonable design guidance (e.g., placement, style suggestions) or flags the need for specific assets.
    *   **Integration Verification:**
        *   Check JIRA links (Epic -> Story -> Task -> Bug, Epic -> Confluence).
        *   Check Confluence page structure/content against templates and FUAM principles.
        *   Verify Git commits are associated with the correct JIRA Task key (via commit message convention).

**2. Quality & Maintainability Assessment:**

*   **Goal:** Is the output (code, docs, tickets) of high quality and easy for humans (or other agents) to work with?
*   **Methods:**
    *   **Code Quality Metrics:**
        *   Run static analysis tools (SonarQube, linters like ESLint/Pylint/Checkstyle, complexity checkers). Track metrics like complexity score, code smells, duplication.
        *   Run unit test coverage tools. Aim for a target coverage percentage generated by the Coder/Tester.
    *   **Documentation Quality (FUAM):**
        *   **Findable:** Is the Confluence page placed correctly? Is it linked from JIRA?
        *   **Understandable:** Is the language clear? Is the structure logical (headings, lists)? Are diagrams included where appropriate? (Partially automatable with keyword checks, largely manual).
        *   **Actionable:** Does the spec/task clearly state what needs to be done? Are ACs clear? (Manual review).
        *   **Maintainable:** Is the documentation easy to update? Does it use templates? (Manual review).
    *   **JIRA Ticket Quality:**
        *   Are titles concise and descriptive?
        *   Are descriptions clear and provide sufficient context?
        *   Are Acceptance Criteria specific, measurable, achievable, relevant, and time-bound (SMART)? (Manual review).
    *   **Human Review:** Periodically have experienced developers/PMs review samples of generated code, documentation, and JIRA tickets against established team standards. Use checklists or rubrics.

**3. Cost Efficiency (Token Usage) Benchmarks:**

*   **Goal:** Is MIDAS achieving tasks with minimal LLM token consumption?
*   **Methods:**
    *   **Baseline Measurement:** Run standard test scenarios (like the simple E2E) and record total input/output tokens consumed by all agents involved.
    *   **Comparative Analysis:**
        *   Run the same task with different context management strategies enabled/disabled (e.g., with/without summarization, RAG vs. full context). Compare token usage.
        *   Run the same task targeting different LLMs (if RooCode supports model selection per agent/task). Compare cost vs. quality.
        *   Compare token usage for a large task handled monolithically vs. decomposed by Planner/PO first.
    *   **Track Trends:** Monitor token usage per feature or over time to identify regressions or improvements.

**4. Robustness & Error Handling Tests:**

*   **Goal:** Does MIDAS handle failures, ambiguities, and edge cases gracefully?
*   **Scenarios:**
    *   **Tool Failure Simulation:** Intentionally configure `mcp-atlassian` or `GitMCP` with bad credentials or point to non-existent resources. Verify the relevant agent detects the error, reports it clearly (e.g., updates JIRA task with error, notifies user), and doesn't crash RooCode.
    *   **Ambiguous Instructions:** Give a vague feature request. Does the Planner ask clarifying questions (via a HITL mechanism or JIRA comment) or hallucinate a poor solution?
    *   **Conflicting Information:** Provide contradictory requirements in the initial spec or between a JIRA task and Confluence page. How does the relevant agent resolve or flag the conflict?
    *   **Token Limit Stress Test:** Give a task requiring analysis of very large files or extensive history. Observe if context management strategies (summarization, RAG, decomposition, HITL trigger) activate correctly.
    *   **Infinite Loop Potential:** Craft a task prone to looping (e.g., self-referential refactoring without clear termination). Check if any loop detection mechanisms (if implemented) trigger or if it consumes excessive resources/time before failing.

**5. Collaboration & Workflow Efficiency:**

*   **Goal:** Do agents communicate and hand off tasks effectively without significant delays or information loss?
*   **Methods:**
    *   **Trace Execution:** Use RooCode's logging/tracing to visualize the flow of control between agents (like the sequence diagram). Check for unnecessary back-and-forth or bottlenecks.
    *   **Context Handoff Verification:** After a handoff (e.g., Planner -> PO), verify the receiving agent has the necessary context (e.g., correct Epic keys) to proceed without re-asking the LLM for readily available information passed by the previous agent.
    *   **Dependency Handling:** Create tasks with explicit dependencies in JIRA. Verify the system respects these dependencies (e.g., Coder doesn't start Task B until dependent Task A is marked 'Done' or 'Ready for Dev').
    *   **Measure Turnaround Time:** For standard workflows, measure the total time from initial request to final completion, breaking it down by agent phases to identify slow points.
